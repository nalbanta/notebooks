{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Scikit-Learn:<br/>Trees and Forests\n",
    "## Jake VanderPlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in these cells is runable. \n",
    "# Click on this cell, then press Shift+Enter to run it, \n",
    "# or click the Run button in the toolbar.\n",
    "\n",
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "This notebook gives a conceptual introduction to decision trees and random forests, along with some examples of their usage in scikit-learn.\n",
    "\n",
    "I will begin by exploring the idea of a simple decision tree for classification. After discussing the drawbacks of decision trees, I’ll explore the idea of instead using randomized collections of decision trees, known as random forests. The main benefit of random forests is the ability to fit complicated datasets while avoiding the problem of overfitting.\n",
    "\n",
    "At the end of this notebook, the viewer will:\n",
    "\n",
    "- Have an intuitive understanding of decision tree and random forest models\n",
    "- Understand the term bagging and what it means in a machine learning context\n",
    "- Understand how to use decision trees and random forests in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we'll start with imports and enable matplotlib's ``inline`` mode, so that figures display in the notebook rather than in a new window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also use the Seaborn style for our plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivating Random Forests: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are an example of an *ensemble learner* built on decision trees.\n",
    "For this reason, we'll start by discussing decision trees themselves.\n",
    "\n",
    "Decision trees are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero in on the classification.\n",
    "For example, if you wanted to build a decision tree to classify an animal you come across while on a hike, you might construct the one shown here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.08-decision-tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary splitting makes this algorithm very efficient. In a well-constructed tree, each question will cut the number of options by approximately half, very quickly narrowing the options even among a large number of classes.\n",
    "\n",
    "The trick, of course, is in deciding which questions to ask at each step!\n",
    "\n",
    "In machine learning implementations of decision trees, the questions generally take the form of axis-aligned splits in the data: that is, each node in the tree splits the data into two groups using a cutoff value within one of the features.\n",
    "Let's now look at an example of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Decision Tree\n",
    "\n",
    "Consider the following two-dimensional data, which has one of four class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                  random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple decision tree built on this data will iteratively split the data along one or the other axis according to some quantitative criterion, and at each level assign the label of the new region according to a majority vote of points within it.\n",
    "\n",
    "The following widget shows an interactive decision tree for this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lesson2\n",
    "lesson2.plot_tree_interactive(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that after the first split, every point in the upper branch remains unchanged, so there is no need to further subdivide this branch.\n",
    "Except for nodes that contain all of one color, at each level *every* region is again split along one of the two features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process of fitting a decision tree to our data can be done in scikit-learn with the ``DecisionTreeClassifier`` estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the utility function in ``lesson2.py`` lets us visualize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lesson2\n",
    "lesson2.visualize_classifier(tree, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees and Overfitting\n",
    "\n",
    "Notice that as the depth increases, we tend to get very strangely shaped classification regions; for example, at a depth of five, there is a tall and skinny purple region between the yellow and blue regions.\n",
    "It's clear that this is less a result of the true, intrinsic data distribution, and more a result of the particular sampling or noise properties of the data.\n",
    "\n",
    "Such overfitting turns out to be a general property of decision trees: it is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions they are drawn from.\n",
    "Another way to see this overfitting is to look at models trained on different subsets of the data. For example, in this figure we train two different trees, each on half of the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "for i in range(2):\n",
    "    indices = np.random.randint(0, len(X), 3 * len(X) // 2)\n",
    "    tree = DecisionTreeClassifier()\n",
    "    lesson2.visualize_classifier(tree, X[indices], y[indices], ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that in some places, the two trees produce consistent results (e.g., in the four corners), while in other places, the two trees give very different classifications (e.g., in the regions between any two clusters).\n",
    "The key observation is that the inconsistencies tend to happen where the classification is less certain—thus, by using information from *both* of these trees, we might come up with a better result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore this with a widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "# store X & y here so that this widget still works\n",
    "# when X & y are overwritten in later cells\n",
    "randomized_tree_data = (X, y)\n",
    "\n",
    "@interact\n",
    "def fit_randomized_tree(random_state=100, frac=(0.0, 1.0)):\n",
    "    X, y = randomized_tree_data\n",
    "    clf = DecisionTreeClassifier(max_depth=15)\n",
    "    i = np.arange(len(y))\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rng.shuffle(i)\n",
    "    N = int(frac * X.shape[0])\n",
    "    lesson2.visualize_tree(clf, X[i[:N]], y[i[:N]], boundaries=False,\n",
    "                           xlim=(X[:, 0].min(), X[:, 0].max()),\n",
    "                           ylim=(X[:, 1].min(), X[:, 1].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as using information from two trees improves our results, we might expect that using information from many trees would improve our results even further; we'll explore this idea in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles of Estimators: Bagging and Random Forests\n",
    "\n",
    "The notion that multiple overfitting estimators can be combined to reduce the effect of overfitting is what underlies an ensemble method called *bagging*.\n",
    "Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of which overfits the data, and averages the results to find a better classification.\n",
    "An ensemble of randomized decision trees is known as a *random forest*.\n",
    "\n",
    "This type of bagging classification can be done manually using scikit-learn's ``BaggingClassifier`` meta-estimator, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(model, n_estimators=100, max_samples=0.8,\n",
    "                        random_state=1)\n",
    "\n",
    "bag.fit(X, y)\n",
    "lesson2.visualize_classifier(bag, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have randomized the data by fitting each estimator with a random subset of 80% of the training points.\n",
    "In practice, decision trees are more effectively randomized by injecting some stochasticity in how the splits are chosen: this way all the data contributes to the fit each time, but the results of the fit still have the desired randomness.\n",
    "For example, when determining which feature to split on, the randomized tree might select from among the top several features.\n",
    "You can read more technical details about these randomization strategies in the <a href=\"http://scikit-learn.org/stable/modules/ensemble.html#forest\" target=\"_blank\">scikit-learn documentation and references within</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, such an optimized ensemble of randomized decision trees is implemented in the ``RandomForestClassifier`` estimator, which automatically takes care of all the randomization.\n",
    "All you need to do is select a number of estimators, and it will very quickly (in parallel, if desired) fit the ensemble of trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "lesson2.visualize_classifier(model, X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that by averaging over 100 randomly perturbed models, we end up with an overall model that is much closer to our intuition about how the parameter space should be split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression\n",
    "\n",
    "Earlier, we considered random forests within the context of classification.\n",
    "\n",
    "Random forests can also be made to work in the case of regression (that is, continuous rather than categorical variables). The estimator to use for this is the ``RandomForestRegressor``, and the syntax is very similar to what we saw above.\n",
    "\n",
    "Consider the following data, drawn from the combination of a fast and slow oscillation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(200)\n",
    "\n",
    "def model(x, sigma=0.3):\n",
    "    fast_oscillation = np.sin(5 * x)\n",
    "    slow_oscillation = np.sin(0.5 * x)\n",
    "    noise = sigma * rng.randn(len(x))\n",
    "\n",
    "    return slow_oscillation + fast_oscillation + noise\n",
    "\n",
    "y = model(x)\n",
    "plt.errorbar(x, y, 0.3, fmt='o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the random forest regressor, we can find the best fit curve as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest = RandomForestRegressor(200)\n",
    "forest.fit(x[:, None], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = forest.predict(xfit[:, None])\n",
    "ytrue = model(xfit, sigma=0)\n",
    "\n",
    "plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)\n",
    "plt.plot(xfit, yfit, '-r');\n",
    "plt.plot(xfit, ytrue, '-k', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the true model is shown in the smooth black curve, while the random forest model is shown by the jagged red curve.\n",
    "As you can see, the nonparametric random forest model is flexible enough to fit the multiperiod data, without us needing to specifying a multiperiod model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Random Forest for Classifying Digits\n",
    "\n",
    "In an earlier notebook, *Machine Learning with Scikit-Learn: Introduction to Machine Learning*, we took a quick look at the handwritten digits data.\n",
    "Let's use that again here to see how the random forest classifier can be used in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remind us what we're looking at, we'll visualize the first few data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up the figure\n",
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# plot the digits: each image is 8x8 pixels\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    \n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly classify the digits using a random forest as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target)\n",
    "model = RandomForestClassifier()\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypred = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the accuracy on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for good measure, plot the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "mat = confusion_matrix(ytest, ypred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='Blues')\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that a simple, untuned random forest results in a very accurate classification of the digits data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any ensemble estimator, one free parameter you can tune is the number of estimators.\n",
    "Let's quickly look at how this affects the results in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n_estimators in [1, 5, 10, 50, 100, 500, 1000]:\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    ypred = model.predict(Xtest)\n",
    "    \n",
    "    print(n_estimators, metrics.accuracy_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that a larger ``n_estimators`` value, in this case, leads to a better accuracy on the test data.\n",
    "\n",
    "Similarly, we could explore the effect of ``max_depth``, which controls how many splits each individual tree will have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for max_depth in [1, 2, 4, 8, 16, 32]:\n",
    "    model = RandomForestClassifier(n_estimators=1000,\n",
    "                                   max_depth=max_depth)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    ypred = model.predict(Xtest)\n",
    "    \n",
    "    print(max_depth, metrics.accuracy_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in general, deeper trees lead to better results.\n",
    "\n",
    "These parameters, ``n_estimators`` and ``max_depth``, are known as *hyperparameters* (parameters that control the model itself, rather than parameters fit by the model).\n",
    "In the next notebook, *Machine Learning with Scikit-Learn: Hyperparameters and Model Validation*, we will look in more detail at the process of choosing optimal hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, we looked at decision trees (very simple models that use binary splits to quickly predict a label on unknown data) and random forests (a collection of randomized decision trees).\n",
    "Random forests are a powerful estimator because they are both fast to train and evaluate, and can closely fit very complicated datasets."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
