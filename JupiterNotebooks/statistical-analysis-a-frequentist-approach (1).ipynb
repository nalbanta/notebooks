{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe173da98029bbe0d185d51ebc65f1c74514a797"
   },
   "source": [
    "<h1 align=\"center\">Statistical Analysis</h1>\n",
    "\n",
    "\n",
    "\n",
    "<h2>Before Starting</h2>\n",
    "Just want to note that this work is not finished yet and will be constantly updated, I find that having a basic understanding in the field of statistics is essential for data scientists, so I decided to share a bit of that basic knowledge with the Kaggle community. If you would like to see more, upvote so I know that the community is interested in understanding the essential concepts of Statistics. <br><br>\n",
    "\n",
    "<h2> Introduction</h2> \n",
    "In this project we will explore the basics of statistical analysis and why it is important to use it in subjects such as data science. When I started working with kernels, I always heard about concepts such as <b>outliers</b>, <b>Interquartile Range (IQR)</b>, and <b>hypothesis testing</b> among many others. My main aim in this simple statistical tutorial is finding out why we need to know such concepts when performing statistical analysis. I started learning R a couple of months ago and I have found that performing this task in R tends to be more intuitive. However, Python has really good libraries such as `stasmodels` or the `scipy stats` library.\n",
    "\n",
    "\n",
    "\n",
    "<h3>Outline</h3>\n",
    "\n",
    "I. <b>Statistics Basics</b> <br>\n",
    "a) [Summaries and Histograms](#summary) <br>\n",
    "b) [The Relationship Between Mean and Median](#mean_median) <br>\n",
    "c) [Boxplots and Suspected Outliers](#boxplots) <br>\n",
    "d) [Using Natural Logarithms and Qplots](#qplots) <br><br>\n",
    "\n",
    "\n",
    "II. <b>Inferential Statistics</b> <br>\n",
    "a) [Hypothesis Testing and Confidence Intervals](#hypothesis_testing) <br>\n",
    "b) [Interpreting Confidence Intervals](#confidence_intervals) <br>\n",
    "c) [Significance Levels and P-Values.](#p_val) <br>\n",
    "d) [Brief Introduction to Chi-Squared Test of Independence (will be continued)](#chi_s) <br>\n",
    "e) [Contingency Tables for Categorical Variables](#contingency)<br>\n",
    "f) [T-Distribution](#t_distribution) <br>\n",
    "g) [Difference Between Two Independent Means](#two_independent)<br>\n",
    "h) [Difference Between More than Two Independent Mean (ANOVA)](#anova) <br>\n",
    "i) [Bootstrapping (Simulation Base Method)](#bootstrapping)<br><br>\n",
    "\n",
    "\n",
    "III.  Other Statistics <br>\n",
    "a) [Mosaic Plots and Contingency Tables](#mosaic)\n",
    "\n",
    "\n",
    "<h3>References</h3>\n",
    "a) <a href=\"http://www.statisticshowto.com/\">Statistics Terminology</a><br>\n",
    "b) <a href=\"https://www.kaggle.com/ruslankl/bio-statistics-in-r-part-1\">(Bio)statistics in R: Part #1\n",
    " </a> by def me <br>\n",
    " c) <a href=\"https://www.coursera.org/specializations/statistics\">Statistics with R</a> Coursera Certification by Duke University <br>\n",
    " d) <a href=\"https://www.youtube.com/watch?v=TP6r5CTd9yM\">Performing the Non-parametric Bootstrap for statistical inference using R</a> by Ian Dworkin <br>\n",
    " e) <a href=\"https://www.datacamp.com/tracks/data-visualization-with-r\">Data visualization with ggplot Part 2 </a> by Rick Scavetta <br>\n",
    " f) <a href=\"https://stats.stackexchange.com/questions/149219/what-is-the-definition-of-expected-counts-in-chi-square-tests\">Stack Exchange Reply</a> by Penguin_Knight<br>\n",
    " g) <a href=\"http://rpubs.com/zach_loertscher/406399\">Outlier Detection</a> by Zach Loertscher\n",
    " \n",
    " \n",
    " <h3>Special Thanks</h3>\n",
    " Special thanks to <b>Alexander Geiger</b> from Golden Oak Research for uploading this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fafbff29-1674-4c25-aca1-360f9073ac3a",
    "_execution_state": "idle",
    "_uuid": "ede6e925957216cc55d8c23789a53a909a453684"
   },
   "outputs": [],
   "source": [
    "# Loading Libraries\n",
    "if (!require(\"pacman\")) install.packages(\"pacman\") \n",
    "pacman::p_load(tidyverse, skimr, GGally, plotly, viridis, caret, randomForest, e1071, rpart, \n",
    "               xgboost, h2o, corrplot, rpart.plot, corrgram, lightgbm, ggplot2, highcharter, \n",
    "               ggthemes, psych, scales, treemap, treemapify, repr, cowplot, magrittr, ggpubr,\n",
    "               RColorBrewer, plotrix, ggrepel, tidyverse, gridExtra, reshape2, janitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d74fb1ab-3a7e-42a2-8d87-9f44e8e01f6a",
    "_uuid": "9ca9f59203cf5551f5f716612da5170f139e33a0"
   },
   "outputs": [],
   "source": [
    "df <- read.csv('../input/real_estate_db.csv')\n",
    "\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "272fcdd99ef0d6ce65dcd02bb051c7cb63ebdcf7"
   },
   "source": [
    "<a id=\"summary\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7f20003e92b4809f9a4b6214f8d420c0551fbfd"
   },
   "outputs": [],
   "source": [
    "summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5ff2bb5b-b590-4e2e-9a59-081b7a2cdcc6",
    "_uuid": "c5e4fabdaa7cfffdbba9f83e3b44676c06233365"
   },
   "outputs": [],
   "source": [
    "numerics <- select_if(df, is.numeric)\n",
    "colnames(numerics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f16e51a2ac57482f48343832d013998a3c84f2a"
   },
   "source": [
    "<h4> Types of Distributions </h4>\n",
    "<a id=\"mean_median\"></a>\n",
    "<ul> \n",
    "<li><b> Normal Distribution: </b> Also known as bell curve, this is a distribution in which half of the data lies on the left side and the other half lies on the right side of the distribution. In this distribution the curve is symmetric and the mean, mode, and median are all equal. </li>\n",
    "<li><b>Right Skewed Distribution:</b> This distribution has a long tail pointing to the <b>right</b>. This means that in our sample or population most of the data is concentrated to the left side of the distribution. <b>The rent mean</b> is, in this case, right-skewed. This tells us that the average rent was mostly concentrated on the left side, meaning that the majority of observations cannot afford a high rent.</li>\n",
    "<li><b>Left Skewed Distribution:</b> This distribution has a long tail pointing to the <b>left</b>. This means that in our sample or population most of the data is concentrated on the right side of the distribution. <b>Debt</b> is an example of a left-skewed distribution, meaning that most observations have a high concentration of debt, and also meaning that most observations are on the right side. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ddc7d31213b8e0f5bfd2a9d4194bc6ddc785ce2"
   },
   "outputs": [],
   "source": [
    "# What is the distribution of the family mean\n",
    "\n",
    "options(repr.plot.width=8, repr.plot.height=7)\n",
    "\n",
    "# numerics %>% filter(!is.na(family_mean)) %>% \n",
    "#   summarize(mean=mean(family_mean), sd=sd(family_mean))\n",
    "\n",
    "\n",
    "subset.rent <- numerics %>%\n",
    "  filter(!is.na(rent_mean))\n",
    "\n",
    "p1 <- ggplot(data=subset.rent, aes(x=rent_mean))+\n",
    "  geom_histogram(aes(y=..density..), bins = 40, fill=\"#81F781\")+\n",
    "  stat_function(fun=dnorm, color=\"black\",\n",
    "                args=list(mean=mean(subset.rent$rent_mean), \n",
    "                          sd=sd(subset.rent$rent_mean))) + theme_minimal() + \n",
    "theme(plot.title=element_text(hjust=0.5)) + labs(title=\"Right Skewed Distribution\", \n",
    "                                                x=\"Rent Mean\", y=\"Probability\")\n",
    "\n",
    "\n",
    "subset.female <- numerics %>%\n",
    "  filter(!is.na(female_age_mean))\n",
    "\n",
    "\n",
    "p2 <- ggplot(data=subset.female, aes(x=female_age_mean))+\n",
    "  geom_histogram(aes(y=..density..), bins = 40, fill=\"#FAAC58\")+\n",
    "  stat_function(fun=dnorm, color=\"black\",\n",
    "                args=list(mean=mean(subset.female$female_age_mean), \n",
    "                          sd=sd(subset.female$female_age_mean))) + theme_minimal() + \n",
    "theme(plot.title=element_text(hjust=0.5)) + labs(title=\"Normal Distribution\", \n",
    "                                                x=\"Female Age Mean\", y=\"Probability\")\n",
    "\n",
    "\n",
    "subset.debt <- numerics %>%\n",
    "  filter(!is.na(debt))\n",
    "\n",
    "\n",
    "p3 <- ggplot(data=subset.debt, aes(x=debt))+\n",
    "  geom_histogram(aes(y=..density..), bins = 40, fill=\"#FA5858\")+\n",
    "  stat_function(fun=dnorm, color=\"black\",\n",
    "                args=list(mean=mean(subset.debt$debt), \n",
    "                          sd=sd(subset.debt$debt))) + theme_minimal() + \n",
    "theme(plot.title=element_text(hjust=0.5)) + labs(title=\"Left Skewed Distribution\", \n",
    "                                                x=\"Debt\", y=\"Probability\")\n",
    "\n",
    "plot_grid(p1, p2, p3, align='h', nrow=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5cb5e125495704858c1b408afbddca6289ae6ddf"
   },
   "outputs": [],
   "source": [
    "# Summary Statistics\n",
    "cols <- numerics %>% select(debt, rent_mean, female_age_mean) %>% \n",
    "filter(!is.na(debt), !is.na(rent_mean), !is.na(female_age_mean))\n",
    "\n",
    "do.call(cbind, lapply(cols, summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c470c56250a7a94fc1e2af6872ea7536dc3fa35e"
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=4)\n",
    "# windows(height = 7, width = 3.5)\n",
    "# Lines: Mean is the blue line and Median the green line\n",
    "\n",
    "# First Subplot\n",
    "p4 <- hist(subset.rent$rent_mean, col=\"#F78181\", xlab=\"Rent\", main=\"Distribution of Rent\")\n",
    "abline(v = mean(subset.rent$rent_mean), col = \"blue\", lwd = 2, lty=\"dashed\")\n",
    "abline(v = median(subset.rent$rent_mean), col = \"green\", lwd = 2, lty=\"dashed\")\n",
    "legend(x = c(4000, 3200), y = c(8000, 5500), legend=c(\"Mean\", \"Median\"), col=c(\"blue\",\"green\"), cex=0.7, \n",
    "      lty=\"dashed\", lwd=1, y.intersp = 3.8, x.intersp=3.5, xjust=-1.8)\n",
    "\n",
    "\n",
    "# Second Subplot\n",
    "p5 <- hist(subset.female$female_age_mean, col=\"#F78181\", xlab=\"Age\", main=\"Distribution of Female Age\")\n",
    "abline(v = mean(subset.female$female_age_mean), col = \"blue\", lwd = 2, lty=\"dashed\")\n",
    "abline(v = median(subset.female$female_age_mean), col = \"green\", lwd = 2, lty=\"dashed\")\n",
    "legend(x = c(78, 95), y = c(12000, 8000), legend=c(\"Mean\", \"Median\"), col=c(\"blue\",\"green\"), cex=0.7, \n",
    "      lty=\"dashed\", lwd=1, y.intersp = 3.8, x.intersp=3.5, xjust=-1.8)\n",
    "\n",
    "# Third Subplot\n",
    "p6 <- hist(subset.debt$debt, col=\"#F78181\", xlab=\"Debt\", main=\"Distribution of Debt\")\n",
    "abline(v = mean(subset.debt$debt), col = \"blue\", lwd = 2, lty=\"dashed\")\n",
    "abline(v = median(subset.debt$debt), col = \"green\", lwd = 2, lty=\"dashed\")\n",
    "legend(x = c(0.85, 1), y = c(5000, 3500), legend=c(\"Mean\", \"Median\"), col=c(\"blue\",\"green\"), cex=0.8, \n",
    "      lty=\"dashed\", lwd=1, y.intersp = 2, x.intersp=0.7, xjust=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f1a3fd00ccd78867adfdc27d0d6d7309e1aae97"
   },
   "source": [
    "<h4> Other Statistical Measures</h4>\n",
    "<ul>\n",
    "<li><b>Variance:</b> This is an indicator of how spread out our data is. The smallest variabilitiy there could be is 0, while the biggest is infinite. Variance is expressed as: <br> $\\sigma^2 = \\frac{\\sum\\limits_{i=1}^N (X -\\mu)^2}{N}$ </li><br>\n",
    "<li><b>Standard Deviation:</b> The standard deviation is just the square root of our variance and it tells us how far our data is spread from the mean. Standard deviation is expressed as: <br>\n",
    "$s = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\overline{x})^2}$</li><br>\n",
    "<li><b>1st Quartile</b>: This is comprised of the lowest 25% of numbers in our distribution.  </li>\n",
    "<li><b>2nd Quartile (Q2)</b>: Comprised of 50% of lowest numbers up to the <b>median.</b> </li>\n",
    "<li><b>3rd Quartile (Q3)</b>: Comprised of 75% of lowest numbers. </li>\n",
    "<li><b> Interquartile Range (IQR)</b>: This helps us detect where most of the data lies. IQR is expressed as: <br>\n",
    "IQR = $Q_1 - Q_3$  <br>\n",
    "It's preferable to use IQR instead of the mean or median when trying to find out where most of the data lies.</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c42ff4dba0bdeb2f20c67a8f562118f0da0aa386"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c3caab8e758099f2f7f6c368ea3d14dd418724e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4263cd1d105ea97af51657a5c3db0d12a67f697"
   },
   "outputs": [],
   "source": [
    "# We will use the male population now\n",
    "stat_rent <- numerics %>% filter(!is.na(rent_mean)) %>%\n",
    "  summarise(mu = mean(rent_mean), rent_med = median(rent_mean), \n",
    "            std = sd(rent_mean), \n",
    "            rent_min = min(rent_mean), rent_max = max(rent_mean),\n",
    "            rent_q1 = quantile(rent_mean, 0.25),  # first quartile, 25th percentile\n",
    "            rent_q3 = quantile(rent_mean, 0.75),# third quartile, 75th percentile\n",
    "           rent_iqr=rent_q3 - rent_q1)  # Interquartile Range\n",
    "\n",
    "stat_fage <- numerics %>% filter(!is.na(female_age_mean)) %>%\n",
    "  summarise(mu = mean(female_age_mean), fage_med = median(female_age_mean), \n",
    "            std = sd(female_age_mean),\n",
    "            fage_min = min(female_age_mean), fage_max = max(female_age_mean),\n",
    "            fage_q1 = quantile(female_age_mean, 0.25),  # first quartile, 25th percentile\n",
    "            fage_q3 = quantile(female_age_mean, 0.75), # third quartile, 75th percentile\n",
    "           fage_iqr=fage_q3 - fage_q1) # Interquartile Range\n",
    "\n",
    "\n",
    "stat_debt <- numerics %>% filter(!is.na(debt)) %>%\n",
    "  summarise(mu = mean(debt), debt_med = median(debt), \n",
    "            std = sd(debt),\n",
    "            debt_min = min(debt), debt_max = max(debt),\n",
    "            debt_q1 = quantile(debt, 0.25),  # first quartile, 25th percentile\n",
    "            debt_q3 = quantile(debt, 0.75), # third quartile, 75th percentile\n",
    "           debt_iqr=debt_q3 - debt_q1)  # Interquartile Range\n",
    "\n",
    "print(\"Rent Mean Statistics\")\n",
    "print(\"---------------------\")\n",
    "kable(stat_rent)\n",
    "print(\"Female Age Mean Statistics\")\n",
    "print(\"---------------------------\")\n",
    "kable(stat_fage)\n",
    "print(\"Debt Statistics\")\n",
    "print(\"---------------\")\n",
    "kable(stat_debt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1f8666eb530c503d53e4e5c2008164bd7f0250c5"
   },
   "source": [
    "<h4>Boxplots and Suspected Outliers</h4>\n",
    "<a id=\"boxplots\"></a>\n",
    "\n",
    "\n",
    "<h4>A Word Regarding Outliers</h4>\n",
    "I just wanted to add that outliers should be carefully analyzed and, although there are common rules such as those in a \"normal distribution,\" any value beyond three standard deviations should be considered an outlier. Even though there is a small probability that a value in a normal distribution is three standard deviations away from the mean, we should carefully analyze as to why this is the case. It could be that the data was mistyped, which will weaken the theory that a specific observation is an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "400fd9b74b349207c8314a580b57af5b60ea019f"
   },
   "outputs": [],
   "source": [
    "# Use boxplots to explain better the concepts of quartiles\n",
    "\n",
    "# We will use type of place\n",
    "t.place <- df %>% select(rent_mean, type) %>% \n",
    "filter(!is.na(rent_mean), !is.na(type)) %>%\n",
    "ggplot(aes(x=type, y=rent_mean)) + geom_boxplot(fill=\"white\", colour=\"black\", \n",
    "                                                outlier.colour = \"red\", outlier.shape = 1) + \n",
    "theme_minimal() + theme(plot.title=element_text(hjust=0.5)) + coord_flip() + \n",
    "labs(title=\"Distribution of Average Rent by Type of Place\", x=\"Type\", y=\"Average Rent\")\n",
    "\n",
    "t.place + scale_fill_manual(values=c(\"#999999\", \"#E69F00\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "799de0a8e9be26662a7bbccac4cce1aa1679c1b6"
   },
   "source": [
    "<h4> Understanding Q-Plots </h4>\n",
    "<a id=\"qplots\"></a>\n",
    "<ul>\n",
    "<li><b>Right Skewed Qplot:</b> When the distribution is right skewed, the observations tend to go above the red line, indicating that the distribution is right-skewed.\n",
    "<li><b>Normal Distribution Qplot:</b> Although some observations are not on the line, most of the observations are on the line, which indicates that the distribution is mostly normal. </li>\n",
    "<li><b>Left Skewed Qplot:</b> Although this distribution is not strongly left skewed, we can see that most observations fall below the red line, indicating that the distribution is left-skewed. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8700db38aa494f0c3af68510f07f9c4c36b2328a"
   },
   "outputs": [],
   "source": [
    "# Right Skew\n",
    "options(repr.plot.width=10, repr.plot.height=8)\n",
    "par(mfrow=c(3,2)) \n",
    "# First subplot (Right Skewed)\n",
    "hist(subset.rent$rent_mean, main=\"Right Skewed Distribution\", xlab=\"Average Rent\", col=\"#81F781\")\n",
    "qqnorm(subset.rent$rent_mean, col=\"blue\")\n",
    "qqline(subset.rent$rent_mean, col=\"red\")\n",
    "\n",
    "# Second subplot (Normal Distribution)\n",
    "hist(subset.female$female_age_mean, main=\"Normal Distribution\", xlab=\"Average Age\", col=\"#FAAC58\")\n",
    "qqnorm(subset.female$female_age_mean, col=\"blue\")\n",
    "qqline(subset.female$female_age_mean, col=\"red\")\n",
    "\n",
    "# Third Subplot\n",
    "hist(subset.debt$debt, main=\"Left Skewed Distribution\", xlab=\"Debt\", col=\"#FA5858\")\n",
    "qqnorm(subset.debt$debt, col=\"blue\")\n",
    "qqline(subset.debt$debt, col=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb1751216a0e9e072ee75c4d7af1e4e245737376"
   },
   "source": [
    "<h3>Using Natural Logarithms to Create a Normal Distributions</h3><br>\n",
    "\n",
    "Although using natural logarithms does not necessarily impact the observations to form a normal distribution (left-skewed example), most of the times it gives us an approximate normal distribution just like in the right-skewed example. <br>\n",
    "\n",
    "<b>Why would we want a normal distribution?</b><br>\n",
    "Although there are many ways to deal with skewedness, most of the statistical techniques assume that the distribution is \"normal.\" Statistical tests such as z, t, and F-tests assume that the mean is \"normally\" distributed. Moreover, it is somewhat simpler to calculate probabilities and to calculate confidence intervals assuming the distribution meets the <b>Central Limit Theorem (CLT)</b> conditions. \n",
    "\n",
    "\n",
    "<b> More on Central Limit Theorem Conditions: </b><br>\n",
    "<a href=\"http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Probability/BS704_Probability12.html\">More on Central Limit Theorem </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "35b154ad6c1434ba4be6a8498228b0905a526cce"
   },
   "outputs": [],
   "source": [
    "# Right Skew\n",
    "options(repr.plot.width=10, repr.plot.height=8)\n",
    "par(mfrow=c(3,2)) \n",
    "# First subplot (Right Skewed)\n",
    "hist(log1p(subset.rent$rent_mean), main=\"Right Skewed Distribution\", xlab=\"Average Rent\", col=\"#81F781\")\n",
    "qqnorm(log1p(subset.rent$rent_mean), col=\"blue\")\n",
    "qqline(log1p(subset.rent$rent_mean), col=\"red\")\n",
    "\n",
    "# Second subplot (Normal Distribution)\n",
    "hist(log1p(subset.female$female_age_mean), main=\"Normal Distribution\", xlab=\"Average Age\", col=\"#FAAC58\")\n",
    "qqnorm(log1p(subset.female$female_age_mean), col=\"blue\")\n",
    "qqline(log1p(subset.female$female_age_mean), col=\"red\")\n",
    "\n",
    "# Third Subplot\n",
    "hist(log1p(subset.debt$debt), main=\"Left Skewed Distribution\", xlab=\"Debt\", col=\"#FA5858\")\n",
    "qqnorm(log1p(subset.debt$debt), col=\"blue\")\n",
    "qqline(log1p(subset.debt$debt), col=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d9126b4ac39586522a6b00861447b092f24165c1"
   },
   "source": [
    "### Visualizing Confidence Intervals with ggplot\n",
    "With confidence intervals we make sure how confident we are of what the true population average is. <b>The wider the error bars, the less certain of what the true mean is. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a31a0756cfeb0c640e49337e800a1b55222e986c"
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=6)\n",
    "subset.female <- df %>% filter(!is.na(female_age_mean))\n",
    "\n",
    "ggplot(data=subset.female, aes(x=type, y=female_age_mean)) + stat_summary(fun.data=mean_cl_normal, color=\"red\") + theme_minimal() + \n",
    "theme(plot.title=element_text(hjust=0.5)) + labs(title=\"CI for Average Female Age by Zone Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a7cf1494596a8f2467a54064513ac2a20996010"
   },
   "outputs": [],
   "source": [
    "# Function to find the female range\n",
    "age_range <- function(x) {\n",
    "  # Change x below to return the instructed values\n",
    "  data.frame(ymin = min(x), # Min\n",
    "             ymax = max(x)) # Max\n",
    "}\n",
    "\n",
    "age_range(subset.female$female_age_mean)\n",
    "\n",
    "\n",
    "# Finding the Interquartile Range\n",
    "# Function to Custom function\n",
    "IQR <- function(x) {\n",
    "  # Change x below to return the instructed values\n",
    "  data.frame(median = median(x), # Median\n",
    "             first_quartile = quantile(x, 0.25), # 1st quartile\n",
    "             third_quartile = quantile(x, 0.75),\n",
    "            interquartile_range=(quantile(x, 0.75) - quantile(x, 0.25))) # 3rd quartile\n",
    "}\n",
    "\n",
    "IQR(subset.female$female_age_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ec82bd13ce53f72493184f0b2163436aa35057d"
   },
   "source": [
    "<h3> Inference Statistics</h3>\n",
    "<a id=\"hypothesis_testing\"></a>\n",
    "In this section we will talk about the importance of confidence intervals and how we can find the confidence interval of a distribution. Moreover, we will go briefly into the topics of hypothesis testing and explain why it is important to know these concepts before even trying to answer the question of our main problem.\n",
    "\n",
    "\n",
    "<h4>Hypothesis Testing (Guilty or Not Guilty?) </h4>\n",
    "Imagine a scenario where an individual is on trial for commiting a murder in the United States. As far as we know, an individual is considered <b>\"innocent\"</b> until proven otherwise. Through this brief example I would like to introduce the concept of <b>Hypothesis Testing</b>. There are two types of hypotheses: the <b>Null Hypothesis</b> and the <b>Alternative Hypothesis.</b><br>\n",
    "\n",
    "\n",
    "<ul> \n",
    "<li> <b> Null Hypothesis ($H_0$): </b> This is the \"status quo,\" in our example the individual who is on trial is <b>innocent</b>. When we want to compare the means of two variables&#x2014;let's say the average income of males and females&#x2014;the Null Hypothesis would be that there is no difference. </li>\n",
    "<li> <b> Alternative Hypothesis ($H_A$): </b> This is going against what the Null Hypothesis was stating. An individual who went on trial is <b>guilty.</b> In our average income by gender example, the average income of males does not equal the average income of females. </li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "<b> Null Hypothesis (Average Income Example): </b><br>\n",
    "$Aincome_M = Aincome_F$ <br><br>\n",
    "\n",
    "<b> Alternative Hypothesis (Average Income Example): </b><br>\n",
    "$Aincome_M \\ne Aincome_F$\n",
    "\n",
    "\n",
    "\n",
    "<h4> Confidence Intervals and P-values</h4> \n",
    "<b>Confidence Intervals (CI):</b> These are how certain a specific value will lie between two specific points. The most common types of confidence intervals are the 90%, 95%, and 99% confidence intervals, although 95% is most commonly used and is the one we will use in this example.<br>\n",
    "\n",
    "<b> P-value:</b> is the probability of ocurrence of a given event. Assuming a confidence interval is 95%, if p-value < $\\alpha$, then we reject the Null Hypothesis in favor of the Alternative Hypothesis. <br>\n",
    "\n",
    "<b> Significance Level:</b> This is the probability of rejecting the Null Hypothesis (also denoted as alpha or $\\alpha$).\n",
    "\n",
    "<b> Finding the Confidence Interval for the Population Mean</b><br><br>\n",
    "\n",
    "$\\overline{x}\\pm z^* s \\frac{s}{\\sqrt{N-1}}$<br>\n",
    "\n",
    "$\\overline{x}$ = Sample Mean <br>\n",
    "z = z-score <br>\n",
    "s = Standard Error <br>\n",
    "N = Sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee295a4955c2634c4558efbf86f3828713b8bf1b"
   },
   "outputs": [],
   "source": [
    "# Let's calculate the 95% Confidence Level of Male_Age_Mean (Normal Distribution)\n",
    "# Cut the x-axis\n",
    "options(repr.plot.width=6, repr.plot.height=3)\n",
    "\n",
    "# First let's see if Male_Age_Mean follows a normal distribution one of the criterias\n",
    "f.age <- numerics %>% select(female_age_mean) %>% filter(!is.na(female_age_mean)) %>%\n",
    "ggplot(aes(x=female_age_mean, y=..density..)) + geom_histogram(bins = 120, fill=\"red\") + \n",
    "theme_bw() + scale_x_continuous(breaks = seq(20, 80, 10),\n",
    "                               limits=c(20, 80)) + labs(title=\"Distribution of Age from Females\") + \n",
    "theme(plot.title=element_text(hjust=0.5))\n",
    "\n",
    "f.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06ff847afb1467d9397a7247906cd7514d30fb56"
   },
   "outputs": [],
   "source": [
    "# Technique 1: Manually using the statistical measures.\n",
    "\n",
    "# Sample size\n",
    "n <- numerics %>% filter(!is.na(female_age_mean)) %>% nrow()\n",
    "print(paste0(\"Number of rows after filtering Null values \", n))\n",
    "# standard deviation\n",
    "std <- numerics %>% filter(!is.na(female_age_mean)) %>% summarise(std=sd(female_age_mean))\n",
    "# Sample mean\n",
    "x_bar <- numerics %>% filter(!is.na(female_age_mean)) %>% summarise(avg=mean(female_age_mean))\n",
    "\n",
    "# standard error 95% confidence level\n",
    "# We used a negative sign to turn it positive\n",
    "serror <- -qnorm(0.025)*std/sqrt(n)\n",
    "\n",
    "lower_interval <- x_bar - serror\n",
    "upper_interval <- x_bar + serror\n",
    "\n",
    "print(paste0(\"The lower interval is \", round(lower_interval,2), \"  Upper interval is: \", round(upper_interval, 2)))\n",
    "\n",
    "\n",
    "f.age <- numerics %>% filter(!is.na(female_age_mean))\n",
    "\n",
    "# Technique #2 using rcompanion library (Not able to use on Kaggle but it is simpler to use)\n",
    "# groupwiseMean(female_age_mean ~ 1, \n",
    "#               data   = f.age, \n",
    "#               conf   = 0.95, \n",
    "#               digits = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "19b7aafd578c0f663e286b2e6d308c29981f36db"
   },
   "source": [
    "<b> So How Do We Interpret This Confidence Level Example? </b><br>\n",
    "<a id=\"confidence_intervals\"></a>\n",
    "95% of random samples from a sample size of 38,728 (`female_age_mean` without Nulls) of female Americans will yield confidence intervals that capture the true population age mean of females. (40.2 - 40.32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eedb6d2f30348eb4990eec19b9e920dffaab1ef6"
   },
   "outputs": [],
   "source": [
    "# Manually added rep_sample_n function from statsr library\n",
    "rep_sample_n <- function(tbl, size, replace = FALSE, reps = 1)\n",
    "{\n",
    "    n <- nrow(tbl)\n",
    "    i <- unlist(replicate(reps, sample.int(n, size, replace = replace), simplify = FALSE))\n",
    "\n",
    "    rep_tbl <- cbind(replicate = rep(1:reps,rep(size,reps)), tbl[i, , drop=FALSE])\n",
    "\n",
    "    dplyr::group_by(rep_tbl, replicate)\n",
    "}\n",
    "\n",
    "\n",
    "# As we increase sample size the greater the accuracy\n",
    "n <- 50\n",
    "ci_95 <- qnorm(0.975)\n",
    "\n",
    "\n",
    "ci <- f.age %>%\n",
    "  rep_sample_n(size = n, reps = 50, replace = TRUE) %>%\n",
    "  summarise(lower = mean(female_age_mean) - ci_95 * (sd(female_age_mean) / sqrt(n)),\n",
    "            upper = mean(female_age_mean) + ci_95 * (sd(female_age_mean) / sqrt(n)))\n",
    "\n",
    "\n",
    "# Let's see how many times is the actual mean between the lower and upper bounds\n",
    "f.avg <- f.age %>% summarise(avg=mean(female_age_mean))\n",
    "\n",
    "\n",
    "ci <- ci %>%\n",
    "  mutate(capturing_avg = ifelse(lower < f.avg$avg & upper > f.avg$avg, \"In Between\", \"Out of Bounds\"))\n",
    "\n",
    "score <- length(which(ci$capturing_avg == \"In Between\")) / nrow(ci) * 100\n",
    "\n",
    "\n",
    "print(paste0(\"The percentage in which the true average is 'in between': \", score, \"%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "841c0b9bb9befced10e3c48acd9dfb98e0ddda3a"
   },
   "outputs": [],
   "source": [
    "# We need the mean, standard deviation, sample mean, sample size\n",
    "# To make sample mean and standard deviation get the information from the same sample.\n",
    "set.seed(42)\n",
    "mu <- mean(f.age$female_age_mean)\n",
    "n <- 50 # Sample size\n",
    "x_bar <- mean(sample(f.age$female_age_mean, 50))\n",
    "std_bar <- sd(sample(f.age$female_age_mean, 50))\n",
    "standard_error <- qnorm(0.025)*std_bar/sqrt(n) # 95% confidence level\n",
    "\n",
    "# The Z-score\n",
    "z_score <- (x_bar - mu)/standard_error\n",
    "\n",
    "print(paste0(\"Z_score is: \", round(z_score, 2)))\n",
    "\n",
    "z <- (3.2 - 3) / 0.246\n",
    "\n",
    "# p-value for two sides\n",
    "p_val <- 2*pnorm(-abs(z_score))\n",
    "print(paste0(\"Two sided p-value is:  \", round(p_val,2)))\n",
    "\n",
    "# p-value for one side\n",
    "ones_pval <- p_val / 2\n",
    "print(paste0(\"One sided p-value is:  \", round(ones_pval,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "966ac05d6dc3fb3acbb260327eaa310db5d9d10c"
   },
   "source": [
    "<a id=\"p_val\"></a>\n",
    "<b> In this case, since p-value is > than $\\alpha$ we do not reject $H_0$</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8b8b3b5c4a1e39692f08d28692236f4d5dc9cc23"
   },
   "source": [
    "<h4>Contingency Tables</h4>\n",
    "<a id=\"contingency\"></a>\n",
    "The main goal of contingency tables is to summarize the relationship between two categorical variables. In this example, we summarize the relationship between state and type.\n",
    "\n",
    "<h4> Chi-Squared Test of Independence </h4>\n",
    "<a id=\"chi_s\"> </a>\n",
    "<ul>\n",
    "    <li><b>Null Hypothesis($H_0$):</b> There is no association between the two variables. </li>\n",
    "    <li><b>Alternative Hypothesis ($H_A$):</b> There is an association between the two variables and hence, the variables are dependent. </li>\n",
    "</ul>\n",
    "\n",
    "<b> Note:</b> Later I will go more in-depth into the test of independency and what it means. However, I wanted to show a simple way to make sure that independency exists between two variables in order to meet one of the conditions of the <b> Central Limit Theorem.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "287527e2ecf16f0f27a4a70cf578c25527ab5619"
   },
   "outputs": [],
   "source": [
    "categoricals <- select_if(df, is.factor)\n",
    "\n",
    "colnames(categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64b96212f64be6227c61a6980b36953a02bff1de"
   },
   "outputs": [],
   "source": [
    "mytable <- table(categoricals$type, categoricals$state)\n",
    "summary(mytable) # chi-square test of indepedence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf89a4d457b5507eaae52ab3231be35782f3a446"
   },
   "source": [
    "<b>In later updates I will go more in depth into the chi squared test of independence. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c6938a415dd4c8fe71d5ca1c75cdb215947389eb"
   },
   "outputs": [],
   "source": [
    "cont.table <- categoricals %>% select(state, type) %>% table() %>% prop.table() %>% `*` (100) %>% round(2)\n",
    "\n",
    "cont.table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a0c7c36d3de49b7a56a4066cb332d23423d0c380"
   },
   "source": [
    "<ul> \n",
    "    <li><b> First Row:</b> Number of observations.  </li>\n",
    "    <li><b>Second Row:</b> Chi Square contribution. </li>\n",
    "    <li><b> Third Row:</b>Total percentage per row. </li>\n",
    "    <li><b>Fourth Row: </b> Total percentage per column. </li>\n",
    "    <li><b>Fifth Row:  </b>Total percentage of the table. </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "851c7576a5d7a5e3180a9b5aebb9ae9f3bb6e48b"
   },
   "outputs": [],
   "source": [
    "# Contingency Table with CrossTable\n",
    "library(descr)\n",
    "# There might be a problem with the NAS\n",
    "\n",
    "type_state <- df %>% filter(!is.na(type), !is.na(state)) \n",
    "\n",
    "CrossTable(type_state$state, type_state$type, prop.c=TRUE, prop.chisq=TRUE, prop.t=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10302651b438051a38ce4fba8b683612fb0413ed"
   },
   "source": [
    "<h4> T-Distribution </h4>\n",
    "<a id=\"t_distribution\"></a>\n",
    "The t-distribution is a distribution that is only used for small samples. The first question I had when dealing with these types of distribution is why we need a t-distribution when we receive tons of data daily, making it impossible to have a small sample. Well, t-distributions are more often used when <b>conducting an experiment</b> that usually has smaller samples. <br><br>\n",
    "\n",
    "<b>Summary of t-Distribution</b>\n",
    "<ul>\n",
    "    <li><b>Sample size: </b> The sample size must be smaller than 30 in order to be considered for a t-distribution. </li>\n",
    "    <li><b>Degrees of freedom:</b> As the sample size gets closer to 30, the t-distribution will look exactly like a normal distribution. Also, degrees of freedom determines the thickness of the tail. </li>\n",
    "    <li><b>T-score: </b> To calculate the t-score we use the following formula, $(\\overline{x} - Null) / s$ where <i>s</i> is the standard error and Null the Null Hypothesis.</li>\n",
    "    \n",
    "</ul>\n",
    "\n",
    "\n",
    "<b> Excercise 1: Let's find the 95% Confidence Interval of a sample for the <code>rent_mean</code> in the state of New York.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa9bf2668953b0a7aafe4486321d8e9afd6e3e07"
   },
   "outputs": [],
   "source": [
    "# Doing inference with the t-distribution Course 2 Week 3\n",
    "\n",
    "\n",
    "\n",
    "two_states <- df %>% select(state, rent_mean) %>% filter(state == \"California\"| state == \"New York\") %>%\n",
    "filter(!is.na(rent_mean))\n",
    "\n",
    "# On one sample degrees of freedom is (n - 1)\n",
    "set.seed(42)\n",
    "sample_twostates <- sample_n(two_states, 22)\n",
    "\n",
    "# Finding the critical t-score (We always use a positive critical score.)\n",
    "# qt(0.025, df=23)\n",
    "\n",
    "\n",
    "# Let's estimate the rent average for the state of New York using a 95% confidence level\n",
    "ny_samp <- sample_twostates %>% filter(state == \"New York\")\n",
    "\n",
    "# sample size \n",
    "n <- ny_samp %>% nrow()\n",
    "# Sample mean\n",
    "x_bar <- ny_samp %>% summarise(avg=mean(rent_mean))\n",
    "# t-score (df = n - 1)\n",
    "t_score <- abs(qt(0.025, df=11))\n",
    "\n",
    "std_ny <- ny_samp %>% summarise(std=sd(rent_mean))\n",
    "\n",
    "# Find the 95% confidence interval\n",
    "upper_bound <- x_bar + t_score * (std_ny / sqrt(n))\n",
    "lower_bound <- x_bar - t_score * (std_ny / sqrt(n))\n",
    "\n",
    "# We are 95% confident that the  rent mean for the city of new york lies between 934 - 1417"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "66b4984f8cf204acaf054544d394be029cc0344e"
   },
   "source": [
    "<b> Excercise 2: Assume Mu = 1000 for the rent of New York. Let's find the p-value to see if there is sufficient evidence that we could reject $H_0$ in favor of $H_A$. Remember, if the p-value is less than 0.05 significance level, then we reject the Null in favor of the Alternative. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "901919609cc51c70e4cda78a1a7e0e3683707fef"
   },
   "outputs": [],
   "source": [
    "# Let's find the p-value let's assume that the Null is that the Mu = 1000\n",
    "standard_error <- std_ny / sqrt(n)\n",
    "\n",
    "\n",
    "# Let's find the t-score\n",
    "mu <- 1000\n",
    "t_score <- (x_bar -  mu) / standard_error\n",
    "\n",
    "\n",
    "\n",
    "# We have a t-score of 1.605\n",
    "# degrees of freedom = 11\n",
    "\n",
    "# Let's find the p-value\n",
    "# We need the two tails \n",
    "2 * pt(1.605, df=11, lower.tail=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b36d8f4dfd00569e45103a578965310014d3253"
   },
   "source": [
    "Since our p-value is greater than 0.05, we go in favor of the <b>Null Hypothesis</b>, meaning that there is not sufficient evidence that the average rent in New York is something different than 1,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "68ae783b0fc9e0bac3b3d1121adcbbd0ec31101d"
   },
   "source": [
    "<h4> Estimating the Difference Between Independent Means from Two Categorical Variables</h4>\n",
    "<a id=\"two_independent\"></a>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Degrees of Freedom: </b> df = $Min(n_1 - n_2)$ </li>\n",
    "    <li><b>Point of Estimates (Sample Mean): </b>   ($\\overline{x_1} {-} \\overline{x_2}$) </li>\n",
    "    <li><b>Standard Error of difference between two independent means:</b> $SE(_\\overline{x_1}-_\\overline{x_2})$ = $\\sqrt{SE_1^2} + \\sqrt{SE_2^2}$ <br> where $SE_1$ = $\\frac{S_1}{\\sqrt{n_1}}$ and $SE_2$ = $\\frac{S_2}{\\sqrt{n_2}}$</li>\n",
    "    <li><b>Difference between independent means formula:</b> SE = $(\\overline{x_1} - \\overline{x_2})\\pm t^*_{df} SE_{\\overline{x_1} - \\overline{x_2}}$ </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21f6dfe8d5c94e3fc40c1100313563d1a5375d69"
   },
   "outputs": [],
   "source": [
    "# Boxplots of means of florida and texas\n",
    "options(repr.plot.width=8, repr.plot.height=5)\n",
    "\n",
    "\n",
    "south_states <- df %>% select(state, rent_mean) %>% filter(state == \"Texas\" | state == \"Florida\") %>%\n",
    "ggplot(aes(x=state, y=rent_mean, fill=state)) + geom_boxplot() + \n",
    "stat_summary(fun.y=mean, colour=\"orange\", geom=\"point\", size=1) + \n",
    "theme_minimal() + \n",
    "theme(plot.title=element_text(hjust=0.5, size=10)) + \n",
    "labs(title=\"Difference Between Two Independent Means\", y=\"Rent Mean\", x=\"States\") + \n",
    "scale_fill_brewer(palette=\"Set3\")\n",
    "\n",
    "south_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "77a3814f5df499e873536b3c6d3129e6fe57f455"
   },
   "outputs": [],
   "source": [
    "set.seed(42)\n",
    "\n",
    "# Same sample size between the two categories (n = 24)\n",
    "two_states <- df %>% select(state, rent_mean) %>% filter(!is.na(rent_mean)) %>% \n",
    "filter(state == \"Florida\" | state == \"Texas\") %>% group_by(state) %>%\n",
    "do(sample_n(., size = 24))\n",
    "\n",
    "\n",
    "small_sample <- two_states %>% ungroup() \n",
    "\n",
    "\n",
    "# # Sample sizes \n",
    "n_1 <- small_sample %>% filter(state == \"Texas\") %>% nrow()\n",
    "n_2 <- small_sample %>% filter(state == \"Florida\") %>% nrow()\n",
    "\n",
    "# Sample mean for each state\n",
    "xbar_tex <- small_sample %>% filter(state == \"Texas\") %>% summarise(avg=mean(rent_mean))\n",
    "xbar_flo <- small_sample %>% filter(state == \"Florida\") %>% summarise(avg=mean(rent_mean))\n",
    "\n",
    "# Standard deviation\n",
    "std_tex <- small_sample %>% filter(state == \"Texas\") %>% summarise(std=sd(rent_mean))\n",
    "std_flo <- small_sample %>% filter(state == \"Florida\") %>% summarise(std=sd(rent_mean))\n",
    "\n",
    "# Degrees of freedom\n",
    "deg_f <- min(n_1 - 1, n_2 - 1)\n",
    "\n",
    "# T-score\n",
    "t_score <- abs(qt(0.025, df=deg_f))\n",
    "\n",
    "# Standard Error\n",
    "s1 <- std_tex/sqrt(n_1)\n",
    "s2 <- std_flo/sqrt(n_2)\n",
    "\n",
    "std_error <- sqrt(s1)^2 + sqrt(s2) ^ 2\n",
    "\n",
    "# Estimate the difference in rent between Texas and Florida\n",
    "upper_bound <- xbar_tex - xbar_flo + (t_score * std_error)\n",
    "lower_bound <- xbar_tex - xbar_flo - (t_score * std_error)\n",
    "\n",
    "\n",
    "# We are 95% confident that the difference in mean between these two variables range \n",
    "# between -492.34 - 150.55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e071cadae25840d8f497bebbe33678373e512f4b"
   },
   "source": [
    "Time for some <b>Hypothesis Testing</b>. Remember, if the p-value is < 0.05 of our significance level, then we reject the Null Hypothesis in favor of the Alternative Hypothesis. <br>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Null Hypothesis ($H_0$)</b>: There is no difference between average rents in the states of Florida and Texas or $H_{tex} - H_{flo} = 0$ </li>\n",
    "    <li><b>Alternative Hypothesis ($H_A$)</b>: There is a difference between the average rents in the states of Florida and Texas or $H_{tex} - H_{flo} \\neq 0$ </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa9ad8cdfb0c73d167d014eabfc797604a213deb"
   },
   "outputs": [],
   "source": [
    "# (Point of estimate - Null) / Standard Error\n",
    "p_estimate <- (xbar_tex - xbar_flo)\n",
    "\n",
    "t_score <- abs((p_estimate - 0)/std_error)\n",
    "\n",
    "t_score <- as.numeric(t_score)\n",
    "\n",
    "# p-value (> 0.05)\n",
    "# Degrees of freedom is n-1 or 24-1\n",
    "p_value <- 2 * pt(t_score, df=23, lower.tail=FALSE)\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4440f86c9dd8e13563d08ad3a5a88e4ec423957"
   },
   "source": [
    "<b>The p-value indicates that there is no significant evidence that there is a difference between the average rent between the states of Texas and Florida. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10d5339c898957aabbd1eff6b691bd20ef6fef40"
   },
   "source": [
    "<h4> Comparing Different Means from Categorical Variables</h4>\n",
    "<a id=\"anova\"></a>\n",
    "To compare more than two categorical variables we will need <b>F-Statistics</b> and <b> Analysis of Variance (ANOVA) </b><br>\n",
    "\n",
    "\n",
    "<b> Hypothesis Testing: </b>\n",
    "<ul>\n",
    "<li>$H_0$: The mean is the same across the four states mentioned below. </li>\n",
    "<li>$H_A$: At least one pair of means are different from each other.</li>\n",
    "</ul>\n",
    "\n",
    "<b> F-Statistics: </b><br>\n",
    "$\\frac{Variability \\ between \\ groups}{Variability \\ within \\ groups}$<br>\n",
    "\n",
    "<ul> \n",
    "    <li><b> Between Group Variability:</b> Variability that comes within the class (direct correlation with the states).</li>\n",
    "    <li> <b>Within Group Variability: </b> Variability that comes due to other factors. </li>\n",
    "    </ul>\n",
    "    \n",
    "    \n",
    "<ul>\n",
    "    <li><b>Sum of Squares:</b> Measures the total variability of our response variable (ex: Average Rent).\n",
    " $$SSG = \\sum_{i=1}^{n} n_j(y_i - \\overline{y})^2$$ Where <b>$y_i$ </b>= Value of the response variable, $\\overline{y}$ = Grand mean of the response variable (Average rent per state), and $n_j$ = the number of observations in the group. </li>\n",
    "\n",
    " </ul>\n",
    "\n",
    "<b> Summary</b><br>\n",
    "<ul>\n",
    "    <li><b>Sample Size: </b> A sample size of 250 observations is used in this example for each of the states (total of 1,000 observations). </li>\n",
    "    <li><b>P-value: </b> P-value in this case is less that 0.05, which indicates that at least one pair of the states shows enough evidence that the means are different. </li>\n",
    "    <li><b>Adjusted P-Value: </b> The states that have the highest adjusted p-value are Texas and Florida, which is not a surprise since these are the two states that have a similar average rent. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f767c0b4a0c353720ae22191d08fb2001db91d88"
   },
   "outputs": [],
   "source": [
    "# A small summary of the average rent mean per state\n",
    "library(dplyr)\n",
    "\n",
    "all_avg <- df %>% select(state, rent_mean) %>% group_by(state) %>% filter(!is.na(rent_mean)) %>% \n",
    "filter(state == \"New York\" | state == \"California\"| state == \"Florida\" | state == \"Texas\") %>%\n",
    "summarise(avg=mean(rent_mean), Count=n(), std=sd(rent_mean))\n",
    "\n",
    "\n",
    "all_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "34ad5beb301eaf6295948020c3ad1394b95c6706"
   },
   "outputs": [],
   "source": [
    "# Avoid Randomness\n",
    "set.seed(42)\n",
    "\n",
    "# Sampling\n",
    "four_states <- df %>% select(state, rent_mean) %>% filter(!is.na(rent_mean)) %>% \n",
    "filter(state == \"New York\" | state == \"California\"| state == \"Florida\" | state == \"Texas\") %>%\n",
    "group_by(state) %>% do(sample_n(., size=250))\n",
    "\n",
    "ggplot(four_states, aes(x=state, y=rent_mean, fill=state)) + geom_boxplot() + \n",
    "stat_summary(fun.y=mean, colour=\"orange\", geom=\"point\", size=1) + \n",
    "theme_minimal() + theme(plot.title=element_text(hjust=0.5, size=12)) + \n",
    "labs(title=\"Difference in Independent Categorical Means \\n (Sample Size 250)\", x=\"States\", y=\"Average Rent\") + \n",
    "scale_fill_brewer(palette=\"Set3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79b97361e333eeb6043529a565c09cfbd7ad1e31"
   },
   "outputs": [],
   "source": [
    "# ANOVA Testing\n",
    "\n",
    "\n",
    "fours_sample <- four_states %>% ungroup() \n",
    "\n",
    "# Change to short abbreviations \n",
    "levels(fours_sample$state)[levels(fours_sample$state)==\"Texas\"] <- \"Tx\"\n",
    "levels(fours_sample$state)[levels(fours_sample$state)==\"Florida\"] <- \"Fl\"\n",
    "levels(fours_sample$state)[levels(fours_sample$state)==\"California\"] <- \"Ca\"\n",
    "levels(fours_sample$state)[levels(fours_sample$state)==\"New York\"] <- \"Ny\"\n",
    "\n",
    "\n",
    "aov_states <- aov(rent_mean ~ state, data=fours_sample)\n",
    "\n",
    "aov_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1b8c8e11a8db60c431f1ddf14cf2b2506e1d525"
   },
   "outputs": [],
   "source": [
    "\n",
    "summary(aov_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "adb836e62a5f86f2f911c3695ae4d725826f07ca"
   },
   "source": [
    "<b>Expand the summary (p-value less than 0.05) evidence to see that not all means are equal. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f0baa607fdfd4bcca9b60c91779868e96bf4569"
   },
   "outputs": [],
   "source": [
    "attributes(aov_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60099ab09da7666153341a1d2daa95d4aeb4e32d"
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=7, repr.plot.height=4)\n",
    "\n",
    "plot(aov_states$residuals, ylab=\"Residuals\", main=\"Residuals Plot\", col=\"red\", lwd = 0.5)\n",
    "abline(h=0,col=1, lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f22a27081b9e8e7a14268e551869b1c622e8c6c"
   },
   "source": [
    "<b>Texas and Florida have the highest similarity in the average rent between states according to the adjusted p-value (the highest).</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b02a0116294d3ee194052d329322e1d238a2ce10"
   },
   "outputs": [],
   "source": [
    "TukeyHSD(aov_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e99bbb94d505416724af06d33a8ca2009dcb9da"
   },
   "outputs": [],
   "source": [
    "plot(TukeyHSD(aov_states), las=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "911c43423ef7dc28ef200e914f169db11118c57f"
   },
   "source": [
    "### Bootstrapping (Simulation Base-Method)\n",
    "<a id=\"bootstrapping\"></a>\n",
    "We make small samples from a big sample to make inferences about the unknown population. Remember, the population is unknown, the dataset we have is based on samples rather than the whole U.S. population. In this example, we will use as many as 1,000 samples with the size of 750 observations from the big sample in order to infer the median of the average mortgage cost of the population. Something to mention is that the random samples are done with replacement, which means that after the first sample is taken, the second sample observations from the first sample could still be taken.\n",
    "\n",
    "\n",
    "### Key Intakes from Bootstrapping\n",
    "<ul>\n",
    "    <li>Treat the sample as the population (in this case our entire dataset is the population). </li>\n",
    "    <li>Bootstrapping can be used for many purposes, but in this case we are using it to find the confidence intervals. </li>\n",
    "    <li> Bootstrapping is also used when we stack models together to make certain predictions.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "343862e2df5e7281ccda8583a68350f97c85709c"
   },
   "outputs": [],
   "source": [
    "# Mortgage Costs Right Skew\n",
    "subset_mortgage <- df %>% select(hc_mortgage_mean) %>% filter(!is.na(hc_mortgage_mean))\n",
    "\n",
    "\n",
    "ggplot(data=subset_mortgage, aes(x=hc_mortgage_mean)) + geom_histogram(aes(y=..density..), bins=40, fill=\"#800000\") + \n",
    "stat_function(fun=dnorm, color=\"black\",\n",
    "                args=list(mean=mean(subset_mortgage$hc_mortgage_mean), \n",
    "                          sd=sd(subset_mortgage$hc_mortgage_mean))) + theme_minimal() + \n",
    "labs(title=\"Mortgage Cost (Right-Skewed)\") + theme(plot.title=element_text(hjust=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92d75a57194d4757fc003377acfac491afd1d1ab"
   },
   "outputs": [],
   "source": [
    "# Determine which variable we are going to do Bootstrappoing\n",
    "# Watch again the video of Coursera\n",
    "library(boot)\n",
    "\n",
    "# Function to take the median of each sample\n",
    "BootstrapMedian <- function(x=subset_mortgage$hc_mortgage_mean){\n",
    "    x.boot <- sample(x, size=length(1000), replace=T)\n",
    "    median(x.boot)\n",
    "}\n",
    "\n",
    "# median(subset_mortgage$hc_mortgage_mean)\n",
    "BootstrapMedian()\n",
    "sample_boost <- replicate(2000, BootstrapMedian())\n",
    "\n",
    "\n",
    "hist(sample_boost, col=\"#ff4848\", breaks = 25, main=\"Bootstrap Sample Distribution\")\n",
    "abline(v=median(subset_mortgage$hc_mortgage_mean), lwd=2, col=\"blue\", lty=\"dashed\")\n",
    "abline(v=median(sample_boost), lwd=2, col=\"black\", lty=\"dashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1d159497eb674163609041842bf5fcef39bf00c3"
   },
   "outputs": [],
   "source": [
    "# We are 95% confident that the true median of the population is between 839 - 3180 of the mortgage cost.\n",
    "# This is a wide margin.\n",
    "quantile(sample_boost, probs=c(0.025, 0.975))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1e22c815ca2293f551dd6e0c46b47c831efd08e"
   },
   "source": [
    "### Mosaic Plots and Contingency Tables\n",
    "<a id=\"mosaic\"></a>\n",
    "---> Description Later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1edf65ea397c126daf7a26ec096f9d41ee1ee195"
   },
   "outputs": [],
   "source": [
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb061e6cc78b63475bedece97d5635464a408c50"
   },
   "source": [
    "<b> Note:</b> Puerto Rico is a U.S. territory, not a state. Nevertheless, I added it to the region of the Caribbean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2195b9939d00ffa4bf573f860047297129b6d7d3"
   },
   "outputs": [],
   "source": [
    "df$regions <- NA\n",
    "\n",
    "west <- c('CA', 'OR', 'UT','WA', 'CO', 'NV', 'AK', 'MT', 'HI', 'WY', 'ID')\n",
    "south_west <- c('AZ', 'TX', 'NM', 'OK')\n",
    "south_east <- c('GA', 'NC', 'VA', 'FL', 'KY', 'SC', 'LA', 'AL', 'WV', 'DC', 'AR', 'DE', 'MS', 'TN')\n",
    "mid_west <- c('IL','MO', 'MN', 'OH', 'WI', 'KS', 'MI', 'SD', 'IA', 'NE', 'IN', 'ND')\n",
    "north_east <- c('CT', 'NY', 'PA', 'NJ', 'RI','MA', 'MD', 'VT', 'NH', 'ME')\n",
    "\n",
    "df$regions <- with(df, ifelse(state_ab %in% west, \"West\", \n",
    "                              ifelse(state_ab %in% south_west, \"SouthWest\",\n",
    "                                    ifelse(state_ab %in% south_east, \"South East\",\n",
    "                                          ifelse(state_ab %in% mid_west, \"MidWest\",\n",
    "                                                ifelse(state_ab %in% north_east, \"NorthEast\", \n",
    "                                                       \"Caribbean\")))))) \n",
    "\n",
    "\n",
    "unique(df$regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63668096d07e5f89048337f611a292561ac5ab0c"
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=8)\n",
    "\n",
    "subset_rent <- df %>% filter(!is.na(rent_mean))\n",
    "\n",
    "\n",
    "ggplot(subset_rent, aes (x = rent_mean, fill= regions)) +\n",
    "  geom_histogram(aes(y=..density..), binwidth = 1) + scale_fill_brewer(palette=\"Dark2\") +\n",
    "facet_grid(regions~.) + theme_minimal() + theme(legend.position=\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e9236eaef1c8bd135e9fab5582c67ec99b1470b"
   },
   "outputs": [],
   "source": [
    "# Create a COntingency table between the two categorical variables\n",
    "cont_table <- table(df$regions, df$type)\n",
    "\n",
    "# Let's create a frequency table\n",
    "\n",
    "freq_df <- apply(cont_table, 2, function(x) round(x/sum(x),2))\n",
    "                 \n",
    "# Change the structure of our frequency table.\n",
    "melt_df <- melt(freq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a88647d3ddc2f3298e525e4dd1c026ee658a1405"
   },
   "outputs": [],
   "source": [
    "names(melt_df) <- c(\"Regions\", \"Type\", \"Frequency\")\n",
    "\n",
    "\n",
    "ggplot(melt_df, aes(x = Type, y = Frequency, fill = Regions)) +\n",
    "  geom_col(position = \"stack\") +\n",
    "  facet_grid(Regions ~ .) + \n",
    "scale_fill_brewer(palette=\"Dark2\") + theme_minimal() + theme(legend.position=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6a8c2c32225f626233f67a4a272f9931f372a0c4"
   },
   "source": [
    "Mosaic plots help us determine which categorical variables are <b> underrepresented </b> and <b> overrepresented </b> in relation to each other. A chi-squared test will be used later on to examine this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4af9dfaae4355347d9324907ad257c584a66e051"
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=5)\n",
    "\n",
    "conti_df <- as.data.frame.matrix(table(df$regions, df$type))\n",
    "\n",
    "conti_df$groupSum <- rowSums(conti_df)\n",
    "conti_df$xmax <- cumsum(conti_df$groupSum)\n",
    "conti_df$xmin <- conti_df$xmax - conti_df$groupSum\n",
    "# The groupSum column needs to be removed; don't remove this line\n",
    "conti_df$groupSum <- NULL\n",
    "\n",
    "conti_df$regions <- rownames(conti_df)\n",
    "\n",
    "melt_df <- melt(conti_df, id.vars = c(\"regions\", \"xmin\", \"xmax\"), variable.name = \"type\")\n",
    "\n",
    "df_melt <- melt_df %>%\n",
    "  group_by(regions) %>%\n",
    "  mutate(ymax = cumsum(value/sum(value)),\n",
    "         ymin = ymax - value/sum(value))\n",
    "\n",
    "\n",
    "index <- df_melt$xmax == max(df_melt$xmax)\n",
    "df_melt$yposn <- df_melt$ymin[index] + (df_melt$ymax[index] - df_melt$ymin[index])/2\n",
    "\n",
    "\n",
    "df_melt$xposn <- df_melt$xmin + (df_melt$xmax - df_melt$xmin)/2\n",
    "\n",
    "# geom_text for ages (i.e. the x axis)\n",
    "\n",
    "\n",
    "\n",
    "p1<- ggplot(df_melt, aes(ymin = ymin,\n",
    "                 ymax = ymax,\n",
    "                 xmin = xmin,\n",
    "                 xmax = xmax,\n",
    "                 fill = type)) +\n",
    "  geom_rect(colour = \"white\") +\n",
    "  scale_x_continuous(expand = c(0,0)) +\n",
    "  scale_y_continuous(expand = c(0,0)) +\n",
    "  scale_fill_brewer(palette=\"RdBu\") +\n",
    "  theme_minimal() \n",
    "\n",
    "p1 + \n",
    "  geom_text(aes(x = xposn, label = regions),\n",
    "            y = 0.15, angle = 90,\n",
    "            size = 3, hjust = 1,\n",
    "            show.legend = FALSE) + labs(title=\"Mosaic Plot\") + theme(plot.title=element_text(hjust=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0ad53d4c419e75c9e1290f0fb7939c46e52ca690"
   },
   "source": [
    "### Outlier Detection\n",
    "In this section, we will detect possible outliers. Although, I should mention that outliers should be carefully analyzed and individuals should be really careful when it comes to deleting an observation in which that person believes a particular observation is an outlier.\n",
    "\n",
    "---> <b> More will be written on this once I finish with this section. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe71c6a225d22130ab1a65697465f7a90bb2dc2f"
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=3)\n",
    "\n",
    "# Let's look for normal distributed variables\n",
    "fem_age <- df %>% select(female_age_mean) %>% filter(!is.na(female_age_mean)) %>%\n",
    "ggplot(aes(x=female_age_mean)) + geom_histogram(binwidth=1, fill=\"red\") + theme_minimal()\n",
    "\n",
    "rent <- df %>% select(rent_mean) %>% filter(!is.na(rent_mean)) %>%\n",
    "ggplot(aes(x=rent_mean)) + geom_histogram(fill=\"blue\", binwidth=0.02) + \n",
    "scale_x_log10() + theme_minimal()\n",
    "\n",
    "plot_grid(fem_age, rent, align='h', ncol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f895e330a5addc0be0c45ce6b4031ef9797f412"
   },
   "outputs": [],
   "source": [
    "# Bivariate Analysis\n",
    "female_rent <- df %>% select(female_age_mean, rent_mean) %>% filter(!is.na(female_age_mean), !is.na(rent_mean))\n",
    "\n",
    "set.seed(1)\n",
    "small_sample <- female_rent %>% sample_n(100) \n",
    "\n",
    "ggplot(small_sample, aes(x=female_age_mean, y=rent_mean)) + geom_point(col=\"red\") + theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ace0820612ebd737aba48a5920b4b250c7ef0ce9"
   },
   "outputs": [],
   "source": [
    "sample_std <- small_sample %>%\n",
    "  mutate(sd_female = (female_age_mean-mean(female_age_mean))/sd(female_age_mean),  \n",
    "         sd_rent = (rent_mean-mean(rent_mean))/sd(rent_mean))\n",
    "\n",
    "\n",
    "head(sample_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ff25f56246d2e2d32538ba0f1020edc7b3e7534"
   },
   "source": [
    "### Up Next We Will Use These Metrics to Detect \"Possible Outliers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e39aa08ac89e344554fc858545e93703b9790088"
   },
   "source": [
    "### Up Next: Understanding Chi-Squared Tests and Standarized Residuals: This is still being updated\n",
    "\n",
    "Standardized residual = $\\frac{(observed\\ count – expected\\ count)} {\\sqrt{expected\\ count}}$<br>\n",
    "Expected Count = $\\frac{\\sum{data\\ in\\ row\\ + data\\ in\\ col}}{total\\ data}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "413351dc13ca086f311c37778720c98c699bfb38"
   },
   "outputs": [],
   "source": [
    "cont_table <- table(df$type, df$regions)\n",
    "#  Change to dataframe\n",
    "df_cont_table <- as.data.frame.matrix(cont_table)\n",
    "\n",
    "\n",
    "df_cont_table$Total <- colSums(df_cont_table)\n",
    "\n",
    "\n",
    "df_cont_table <- df_cont_table%>% adorn_totals(\"row\")\n",
    "\n",
    "\n",
    "df_cont_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63e10b94e6dde77f2a2476bfb9a58bb0eea8b16c"
   },
   "outputs": [],
   "source": [
    "# This simple test tells us there is a difference between these two columns (Low p-value.)\n",
    "results <- chisq.test(table(df$regions, df$type))\n",
    "\n",
    "resid <- melt(results$residuals)\n",
    "resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11b6ee17d62b773b46664b228d618f9679cfca22"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
